num_heads = 2 # Number of attention heads in encoder/decoder
model = 206 # Dimension of the model
n_hidden = 1024 # Number of hidden units in the feed forward network
layers = 3 # Number of encoder/decoder layers
vocab_size = 35000 # Number of words in the english-german vocabulary
dropout = 0.1 # Dropout rate
max_tokens = 600 # max number of tokens in a batch
batch_size = 128 # Batch size
label_smoothing = 0.1
num_epochs = 60 # Number of epochs
validation_split_ratio = 0.9
max_grad_norm = 1
learning_rate = 0.001 # Learning rate
