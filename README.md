My implementation of the simple transformer from the paper "Attention is All You Need". I have switched out the default positional embedding with rotary embedding from https://github.com/lucidrains/rotary-embedding-torch. Currently untested, and without the code to train and run has not yet been written.

The code for data_processing comes from https://github.com/Montinger/Transformer-Workbench/blob/main/transformer-from-scratch/0-Cleans-Data-and-Tokenize.py 
